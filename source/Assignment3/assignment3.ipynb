{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo máy ảo bằng Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill kernel cũ đi đồng thời tạo kết nối mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện cần thiết và chứng thực"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện liên quan đến việc kết nối Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân tích cảm xúc với LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng ta sẽ dùng mạng LSTM để giải quyết bài toán phân tích cảm xúc (Sentiment Analysis) trên tập dữ liệu văn bản. Nếu nhìn theo kiểu black box, đầu vào của bài toán là một câu hoặc đoạn văn bản và đầu ra là trạng thái tích cực, tiêu cực hay trung hoà (positive - negative - neutral). Trong phạm vi của assignment này, chúng ta chỉ quan tâm đến hai trạng thái cảm xúc là positive và negative.\n",
    "\n",
    "![caption](Images/input_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Góc nhìn Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như chúng ta giữ nguyên định dạng đầu vào là chuỗi ký tự thì rất khó để thực hiện các thao tác biến đổi như tích vô hướng (dot product) hoặc các thuật toán trên mạng neural network như backpropagation. Thay vì dữ liệu đầu vào là một chuỗi, chúng ta cần chuyển đổi các từ trong tập từ điển sang dạng vector số học trong đó có thể thực hiện được các phép toán nêu trên.\n",
    "\n",
    "![caption](Images/word2vec.png)\n",
    "\n",
    "Trong hình minh hoạ ở trên, ta có thể hình dung dữ liệu đầu vào của thuật toán phân tích cảm xúc là một ma trận 16 x D chiều. Trong đó 16 là số lượng từ trong câu và D là số chiều của không gian vector để biểu diễn từ. Để ánh xạ từ một từ sang một vector, chúng ta sử dụng ma trận word embedding như đã thực hiện trong bài Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng tôi sử dụng tập dữ liệu review trên trang Foody với khoảng 30,000 mẫu được gán nhãn. Trong đó có 15,000 mẫu positive và 15,000 mẫu negative. Nguồn: https://streetcodevn.com/blog/dataset. Tập dữ liệu này đã được đính kèm trong thư mục của assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các bước để huấn luyện trên mạng RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có 5 bước chính để giải quyết bài toán phân tích cảm xúc trong văn bản:\n",
    "\n",
    "    1) Huấn luyện một mô hình phát sinh ra vector từ (như mô hình Word2Vec) hoặc tải lên các vector từ tiền huấn luyện.\n",
    "    2) Tạo ma trận ID cho tập dữ liệu huấn luyện\n",
    "    3) Tạo mô hình RNN với các đơn vị LSTM, sử dụng tensorflow\n",
    "    4) Huấn luyện mô hình RNN với dữ liệu ma trận đã tạo ở bước 2\n",
    "    5) Đánh giá mô hình đã huấn luyện với tập test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load tập từ vựng và ma trận word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, để có thể biến đổi một từ thành một vector, chúng ta sử dụng mô hình đã được huấn luyện trước đó (pretrained model). Mô hình đã train trước đó cho tiếng Việt được lấy ở đây: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.vi.300.vec.gz\n",
    "\n",
    "Tuy nhiên, số lượng từ vựng tiếng Việt được huấn luyện rất lớn, khoảng 2M từ. Mỗi từ được biểu diễn dưới dạng một vector 300 chiều. Với kích thước gốc của ma trận word embedding như vậy sẽ gây khó khăn cho việc load dữ liệu cũng như đưa vào thư viện tensorflow để huấn luyện nên chúng tôi đã tối giản lại với số lượng từ tối thiểu để có thể chạy được trên tập dữ liệu review về đồ ăn của Foody.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# LƯU Ý: CẦN PHẢI CHỈNH LẠI ĐƯỜNG DẪN NÀY THÀNH THƯ MỤC CHỨA CÁC FILE ASSIGNMENT3\n",
    "# CHỮ 'drive' có nghĩa là thư mục mặc định của Google drive\n",
    "currentDir = '.'\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để chắc chắn mọi dữ liệu được load lên một cách chính xác, chúng ta cần kiểm tra xem số lượng từ trong từ điển rút gọn và số chiều của ma trận word embedding có khớp với nhau hay không? Trong trường hợp này số từ mà chúng tôi giữ lại là 19,899 và số chiều trong không gian biểu diễn là 300 chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  19899\n",
      "Size of the word embedding matrix:  (19899, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary: ', len(wordsList))\n",
    "print('Size of the word embedding matrix: ', wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec trên một từ đơn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có thể xác định được vector biểu diễn của một từ tiếng Việt. Đầu tiên chúng ta sẽ xác định xem vị trí của từ đó trong wordsList. Sau đó lấy vector ở dòng tương ứng trên trên ma trận wordVectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of `ngon` in wordsList:  14598\n",
      "Vector representation of `ngon` is:  [-2.040e-02 -9.800e-03  2.290e-01 -3.770e-02  5.430e-02 -2.680e-02\n",
      "  2.190e-02 -6.290e-02 -2.200e-02 -1.010e-02  8.300e-03 -8.810e-02\n",
      " -3.630e-02  7.820e-02 -7.780e-02 -4.930e-02 -6.600e-03 -1.026e-01\n",
      " -1.040e-02  5.380e-02  4.100e-02  6.530e-02 -2.770e-02 -6.340e-02\n",
      "  2.270e-02  4.420e-02  3.340e-02 -4.960e-02  8.290e-02 -3.990e-02\n",
      "  3.750e-02  1.800e-02 -1.115e-01 -7.200e-02 -5.060e-02 -1.051e-01\n",
      " -4.560e-02 -1.765e-01 -3.300e-02 -6.800e-03  5.580e-02 -4.180e-02\n",
      "  4.380e-02  4.940e-02  7.400e-03  4.020e-02 -8.850e-02 -9.840e-02\n",
      " -5.210e-02 -5.500e-03  3.730e-02 -8.460e-02 -6.910e-02 -4.980e-02\n",
      " -3.910e-02 -4.980e-02 -8.690e-02  6.100e-03 -5.360e-02 -3.800e-03\n",
      "  1.162e-01 -4.160e-02  5.000e-03 -7.240e-02 -3.320e-02  1.800e-02\n",
      "  1.200e-02 -4.420e-02  1.350e-01  6.580e-02 -1.110e-02  1.960e-02\n",
      "  1.750e-02  2.010e-02  2.200e-03  1.810e-01 -6.610e-02 -6.860e-02\n",
      " -4.690e-02  7.890e-02  6.880e-02 -5.320e-02  2.770e-02  5.710e-02\n",
      " -1.183e-01  4.170e-02 -8.200e-02 -5.900e-02  8.790e-02  9.640e-02\n",
      "  6.000e-02  1.330e-02 -3.640e-02 -1.110e-02 -2.200e-02  1.770e-02\n",
      " -3.420e-02 -4.020e-02  3.590e-02  1.467e-01 -1.730e-02 -2.650e-02\n",
      "  6.400e-02  7.000e-03 -3.930e-02 -5.540e-02 -4.360e-02  8.000e-02\n",
      " -5.480e-02  3.840e-02 -8.330e-02  7.070e-02 -9.100e-03  2.480e-02\n",
      " -7.500e-03  3.030e-02 -6.600e-03 -9.800e-03  7.640e-02 -9.300e-03\n",
      "  2.330e-02 -2.000e-02  5.970e-02 -2.680e-02 -2.000e-02 -9.700e-03\n",
      " -5.310e-02 -9.820e-02 -2.570e-02  2.400e-02 -5.860e-02  1.820e-02\n",
      " -4.280e-02  9.580e-02  3.400e-02 -7.100e-03 -6.200e-03  1.239e-01\n",
      "  4.830e-02 -4.050e-02  4.810e-02 -1.093e-01  1.540e-02 -3.860e-02\n",
      "  1.250e-01 -7.950e-02  6.800e-03  7.420e-02  5.500e-02 -4.130e-02\n",
      " -2.090e-02 -2.250e-02  3.960e-02 -1.086e-01 -2.200e-02 -4.420e-02\n",
      "  1.965e-01 -2.260e-02  1.196e-01  1.200e-02  1.199e-01 -8.700e-03\n",
      "  4.260e-02  3.460e-02 -3.780e-02  1.951e-01 -9.300e-03 -6.260e-02\n",
      "  2.730e-02  7.340e-02  1.800e-03  5.080e-02 -3.470e-02 -9.680e-02\n",
      " -1.278e-01  3.790e-02  6.920e-02 -5.300e-02 -1.020e-01  1.076e-01\n",
      "  1.361e-01 -7.390e-02  9.650e-02 -2.250e-02  1.597e-01 -2.750e-02\n",
      " -4.200e-03  1.032e-01 -4.910e-02 -7.100e-03 -1.840e-02  7.240e-02\n",
      " -2.040e-02 -5.010e-02 -2.000e-04 -4.190e-02 -5.720e-02 -8.000e-03\n",
      "  9.780e-02 -7.130e-02 -6.070e-02 -1.740e-02 -1.290e-02  8.250e-02\n",
      "  6.600e-03 -2.250e-02 -5.100e-02  6.520e-02 -1.870e-02  5.790e-02\n",
      "  1.814e-01 -1.220e-01  4.770e-02  5.300e-02 -4.230e-02  2.139e-01\n",
      " -9.100e-03  1.314e-01 -3.600e-02 -3.780e-02  4.260e-02  3.000e-04\n",
      " -8.200e-02  1.570e-02 -1.380e-02  3.420e-02 -2.080e-02  1.790e-01\n",
      "  5.240e-02 -1.464e-01  6.330e-02  5.620e-02  2.000e-03 -6.490e-02\n",
      "  4.000e-04 -1.310e-02  1.020e-02  6.380e-02 -1.190e-02  2.440e-02\n",
      " -1.430e-02  1.027e-01  3.200e-03 -1.120e-02  8.270e-02  5.690e-02\n",
      "  2.740e-02 -9.800e-02 -3.150e-02 -9.750e-02 -1.660e-02  7.640e-02\n",
      " -4.960e-02 -7.940e-02  1.177e-01 -2.800e-03  6.860e-02 -5.930e-02\n",
      "  7.470e-02  5.790e-02  3.450e-02  5.550e-02 -3.380e-02  1.292e-01\n",
      "  3.840e-02  7.440e-02 -6.450e-02  2.470e-02 -1.810e-02  9.840e-02\n",
      " -1.329e-01 -6.380e-02 -8.360e-02 -3.580e-02  6.500e-03  8.240e-02\n",
      " -6.140e-02 -1.116e-01  2.310e-02  8.070e-02 -1.670e-02  4.150e-02\n",
      " -8.210e-02  6.290e-02 -5.580e-02  2.600e-03 -2.170e-02  3.200e-03\n",
      " -5.500e-03  6.040e-02  2.990e-02 -1.061e-01  5.200e-02  7.560e-02\n",
      "  6.250e-02  1.007e-01 -1.080e-01 -5.420e-02 -6.620e-02  6.080e-02]\n"
     ]
    }
   ],
   "source": [
    "ngon_idx = wordsList.index('ngon')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "ngon_vec = wordVectors[ngon_idx]\n",
    "print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.1: Word2Vec để biểu diễn một đoạn văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nâng cấp hơn so với phiên bản Word2Vec cho từ đơn, phần này chúng ta sẽ biểu diễn một câu dưới dạng một ma trận gồm các vector biểu diễn của từng từ chồng lên nhau.\n",
    "\n",
    "Ví dụ như chúng ta muốn biểu diễn câu \"Món này ăn hoài không biết chán\". Đầu tiên, với mỗi từ trong câu ta sẽ tìm chỉ số tương ứng trong từ điển và lưu vào vector đặt tên là 'sentenceIndexes'. Sau đó, chúng ta có thể sử dụng hàm tra cứu ma trận word embedding của thư viện Tensorflow tf.nn.embedding_lookup để tra các vector tại các chỉ số trong 'sentenceIndexes'. Như vậy nếu chúng ta sử dụng tối đa 10 từ để lưu trữ cho một câu thì ma trận biểu diễn cho câu sẽ là một ma trận kích thước 10 x 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Row index for each word:  [  119  8136  4884 18791 16614 15951  3371     0     0     0]\n",
      "Sentence representation of word vectors:\n",
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10   #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "# TODO 3.1: Gán chỉ số của các từ trong câu và 'sentenceIndexes'\n",
    "sentence = \"Món này ăn hoài không biết chán\"\n",
    "sentence = sentence.lower().split()\n",
    "\n",
    "for idx, word in enumerate(sentence):\n",
    "    sentenceIndexes[idx] = wordsList.index(word)\n",
    "\n",
    "# Các chỉ số 7, 8, 9 của sentenceIndexes  vẫn được gán bằng 0 như cũ\n",
    "print(sentenceIndexes.shape)\n",
    "print('Row index for each word: ', sentenceIndexes)\n",
    "\n",
    "# Ma trận biểu diễn:\n",
    "print('Sentence representation of word vectors:')\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như thực hiện đúng thì vector 'sentenceIndexes' sẽ có giá trị là: [119, 8136, 4884, 18791, 16614, 15951, 3371, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Khảo sát tập dữ liệu huấn luyện và tạo ma trận ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment 3, chúng tôi sử dụng tập dữ liệu lấy từ trang web Foody trên miền dữ liệu liên quan đến ẩm thực. Tập dữ liệu bao gôm 15.000 review tích cực đặt trong thư mục 'positiveReviews' và 15.000 review tiêu cực đặt trong thư mục 'negativeReviews'. Do khối lượng dữ liệu lớn, nếu chúng ta chọn số lượng từ tối đa (maxSeqLength) quá cao thì sẽ bị lãng phí khi biểu diễn ở những câu review quá ngắn. Ngược lại, nếu sử dụng số lượng từ tối đa quá ít thì sẽ bị bỏ lỡ những từ quan trọng giúp cho việc phân tích cảm xúc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 30000\n",
      "The total number of words in the files is 1770824\n",
      "The average number of words in the files is 59.02746666666667\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = [currentDir + '/positiveReviews/' + f for f in listdir(currentDir + '/positiveReviews/') if isfile(join(currentDir + '/positiveReviews/', f))]\n",
    "negativeFiles = [currentDir + '/negativeReviews/' + f for f in listdir(currentDir + '/negativeReviews/') if isfile(join(currentDir + '/negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể sử dụng thư viện Matplot để minh hoạ phân bố về chiều dài của các câu review trong tập dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên biểu đồ histogram ở trên chúng ta có thể thấy là 180 là kết quả tương đối hợp lý. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có cảm nhận rõ hơn về dữ liệu, chúng ta có thể hiển thị một số review bất kỳ như sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive sentence: \n",
      "2 món bán_chạy nhất và là món có hương_vị đặc_trưng của quán : bánh_tráng cuốn chiên và bánh_tráng cuốn sốt me bơ\n",
      "\n",
      "A negative sentence: \n",
      "Thức uống ngon , view tại Vivo khá là đẹp , sạch_sẽ . Sau khi oder và thanh_toán , mình ngồi đợi và đợi chuông báo thì đến lấy thức uống . Khá là hay_ho ^ ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A positive sentence: ')\n",
    "fname = positiveFiles[3] # Randomly select a positive file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "\n",
    "print('A negative sentence: ')\n",
    "fname = negativeFiles[10] # Randomly select a negative file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn hoá văn bản và tách từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tiết kiệm công sức và cũng nằm ngoài phạm vi của khoá học, chúng tôi đã chuẩn bị sẵn tập dữ liệu đã được tách từ. Giữa hai từ có thể ghép lại để tạo thành một khái niệm mới chúng tôi sử dụng ký tự '_' để nối các từ đó. Ví dụ: 'sinh_viên', 'sinh_học'.\n",
    "\n",
    "Chúng tôi chuẩn bị sẵn các hàm chuẩn hoá văn bản nhằm loại bỏ các ký tự đặc biệt. Tham khảo ở hàm 'cleanSentences'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ biểu diễn 30.000 review dưới dạng các chỉ số của các từ. Tập dữ liệu positive và negative sẽ được tính hợp lại thành một ma trận 30000x180. Trong đó 30000 là số lượng review và 180 là số lượng từ tối đa cho một câu. Do bước chuẩn bị này tốn khá nhiều tài nguyên tính toán nên sau khi tính toán xong, chúng ta sẽ lưu lại để sử dụng cho những lần chạy thí nghiệm sau. Ma trận lưu trữ các chỉ số này là: 'ids'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.2: xác định chỉ số của từng từ trong review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này chúng ta sẽ tiến hành tra cứu từng từ trong review, sau đó gán vào ma trận 'ids'. Trong đó chỉ số dòng của ma trận tương ứng với file review, chỉ số cột của ma trận tương ứng với một từ của review. Trường hợp từ nào không có trong tập từ điển thì ta sẽ gán bằng chỉ số của từ 'UNK' (unknow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files are indexed!\n",
      "Negative files are indexed!\n"
     ]
    }
   ],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "nFiles = 0\n",
    "# Index of Unknow word\n",
    "unk_idx = wordsList.index('UNK')\n",
    "\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            # TODO 3.2: Nếu 'word' thuộc tập 'wordsList' thì gán chỉ số của 'word' vào ma trận ids\n",
    "            if word in wordsList:\n",
    "                ids[nFiles, nIndexes] = wordsList.index(word)\n",
    "            # Ngược lại: gán 'unk_idx' vào ma trận ids\n",
    "            else:\n",
    "                ids[nFiles, nIndexes] = unk_idx\n",
    "            \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "\n",
    "print('Positive files are indexed!')\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            # ToDo 3.2: tương tự như trên. Không khác gì hết.\n",
    "            if word in wordsList:\n",
    "                ids[nFiles, nIndexes] = wordsList.index(word)\n",
    "            else:\n",
    "                ids[nFiles, nIndexes] = unk_idx\n",
    "            \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "\n",
    "print('Negative files are indexed!')\n",
    "# Save ids Matrix for future uses.\n",
    "np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [17821 14017 16521 12145 14598 10048 18752  3167  5767 17821 18838 12062\n",
      " 12413  8484 10774  4964 13946 14964 18838 12062 16805  5596  4884  3910\n",
      " 17108 10131 17162 13245 15522 18584 10715 15341  7352 15247  3527 19898\n",
      " 12714 18503 18584 13245  3741   128  2997 18109 18393 19772 15605 18244\n",
      " 14017 16521 18503  4558  4402 17821  5212  1528 14341 15522 18435 12689\n",
      " 11952  4558 19212 17821  5212 14598 18109 18393  5091   255 15605 12399\n",
      "   255 14485 18393   813  4935  2750 14017 16521  9673 11975  4558  5446\n",
      "  2374 10541  1074  2789  4950 12615 13258 17292  1731 12419  3145 17292\n",
      " 10139 12320  4558  3846 11060   255    85   255     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# LƯU Ý: Bước thực hiện trên tương đối mất thời gian.\n",
    "# Trường hợp đã tính toán và lưu ma trận 'ids' rồi thì ta có thể load lên để sử dụng luôn\n",
    "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
    "print('Word indexes of the first review: ', ids[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như quá trình chuyển từ câu dạng văn bảng sang vector các chỉ số trong từ điển ở trên đúng thì ids[0] sẽ nhận giá trị: [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821 ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng hàm lấy dữ liệu train và test theo từng batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây chúng tôi xây dựng các hàm để lấy dữ liệu train và test theo từng batch. Bạn hãy giải thích tại sao lại có các con số 13999, 14999, 15999, 29999 nhé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            # Pick positive samples randomly\n",
    "            num = randint(1,13999)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            # Pick negative samples randomly\n",
    "            num = randint(15999,29999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(13999,15999)\n",
    "        if (num <= 14999):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Xây dựng RNN Model với Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên chúng tôi sẽ khởi tạo các tham số cho mô hình mạng RNN với các cell là các LSTM. Kiến trúc mạng ở đây bao gồm 128 đơn vị cho mỗi lớp, số lượng layer là 2, số lượng phân lớp là 2 và số vòng lặp khi huấn luyện là 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paramters\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để lưu trữ dữ liệu input và ouput, chúng ta sẽ sử dụng hai kiểu dữ liệu placeholder. Một trong những điều quan trọng nhất khi khởi tạo các biến input và output này là xác định kích thước của các tensor. Mỗi output của mạng (hay còn gọi là label) sẽ là một vector one hot với hai giá trị tương ứng với hai loại cảm xúc: [1, 0] cho positive và [0, 1] cho negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/data_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.3: Xác định input và output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khởi tạo hai biến 'inputs' và 'labels' bằng kiểu placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Tensor(\"Placeholder:0\", shape=(64, 180), dtype=int32)\n",
      "Label: Tensor(\"Placeholder_1:0\", shape=(64, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# TODO 3.3: Khởi tạo hai biến 'inputs' và 'labels'\n",
    "inputs = tf.placeholder(shape=(batchSize, maxSeqLength), dtype=tf.int32)\n",
    "labels = tf.placeholder(shape=(batchSize, numClasses), dtype=tf.float32)\n",
    "\n",
    "print(\"Input: {0}\".format(inputs))\n",
    "print(\"Label: {0}\".format(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó tạo dữ liệu word vector từ khối dữ liệu đầu vào với ma trận word embedding. Nếu như quá trình khởi tạo đúng thì sẽ tạo ra các kiểu dữ liệu sau:\n",
    "labels --> Tensor(\"Placeholder:0\", shape=(64, 2), dtype=float32)\n",
    "inputs --> Tensor(\"Placeholder_1:0\", shape=(64, 180), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup/Identity:0\", shape=(64, 180, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors, inputs)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy sau bước này chúng ta đã có dữ liệu để đưa vào mạng mạng các LSTM. Để khởi tạo một LSTM chúng ta sử dụng hàm tf.nn.rnn_cell.BasicLSTMCell. Hàm này cần tham số đầu vào là số lượng đơn vị muốn khởi tạo. Đây chính là một hyperparamter đã được khởi tạo trước đó.\n",
    "Để chống lại việc overfitting, chúng ta sử dụng lớp dropout. \n",
    "\n",
    "Để tăng tính phức tạp cho kiến trúc mạng chúng ta chồng các lớp LSTM lên nhau (Stack LSTM Layers). Trong trường hợp này chúng ta sử dụng 2 lớp LSTM. Việc chồng thêm các lớp LSTM sẽ giúp cho mô hình có khả năng nhớ nhiều thông tin hơn nhưng đồng thời cũng làm tăng số lượng tham số khi huấn luyện. Điều này cũng có nghĩa là sẽ làm tăng thời gian huấn luyện cũng như là cần thêm nhiều dữ liệu hơn.\n",
    "\n",
    "Cuối cùng là đưa toàn bộ dữ liệu đầu vào vào mạng các LSTM sử dụng hàm tf.nn.dynamic_rnn. Chi tiết kiến trúc mạng LSTM sử dụng cho bài tập này được mô tả trong hình sau:\n",
    "\n",
    "![caption](Images/architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(64, 180, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def generate_a_lstm_layer():\n",
    "    # Khởi tạo một LSTM layer với 'lstmUnits' unit sử dụng hàm tf.contrib.rnn.BasicLSTMCell\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=lstmUnits)\n",
    "    # Sau đó tạo một lớp dropout để chống overfitting với hệ số out_keep_prob bằng 0.75\n",
    "    # Sử dụng hàm tf.contrib.rnn.DropoutWrapper\n",
    "    dropout_cell = tf.contrib.rnn.DropoutWrapper(cell=lstm_cell, output_keep_prob=0.75)\n",
    "    return dropout_cell\n",
    "\n",
    "# Sau khi đã có hàm tạo một LSTM Layer, ta sử dụng hàm này để chồng các LSTM lên\n",
    "# Stack các LSTM layer với hàm tf.nn.rnn_cell.MultiRNNCell\n",
    "lstm_stack = tf.nn.rnn_cell.MultiRNNCell([generate_a_lstm_layer() for _ in range(nLayers)])\n",
    "# Feed data variable vào mạng LSTM sử dụng hàm tf.nn.dynamic_rnn\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell=lstm_stack, inputs=tf.expand_dims(tf.cast(inputs,tf.float32), -1), dtype=tf.float32)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi ra khỏi mạng LSTM, biến outputs sẽ là một tensor có kích thước [batchSize x maxSeqLength x lstmUnits], cụ thể là [64 x 180 x 128]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, chúng ta chỉ lấy dữ liệu ở LSTM cell cuối cùng và cho đi qua lớp kết nối đầy đủ để phân loại thành 2 trạng thái. Chỉ số của LSTM cell cuối cùng là 179 (do có 180 cell theo chiều ngang)  nên để có thể lấy được giá trị ta sẽ chuyển vị về tensor có kích thước [maxSeqLength x batchSize x lstmUnits] hay [180 x 64 x 128]. Sử dụng hàm tf.gather để lấy tensor thứ 179 có kích thước [64 x 128] bao gồm 64 mẫu vector 128 chiều. Vector 128 chiều này sẽ được đưa vào lớp fully connected để chuyển đổi về vector 2 chiều tương ứng với 2 trạng thái.\n",
    "\n",
    "Lớp kết nối đầy đủ bao gồm các bộ tham số 'weight' và 'bias' để thực hiện việc dự đoán kết quả. Bước này chính là tạo một lớp Fully Connected như trong sơ đồ kiến trúc mạng LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "\n",
    "# Lấy giá trị output tại LSTM cell cuối cùng\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "last = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "# Đưa qua mạng Fully Connected mà không có activation function\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để xác định độ chính xác của hệ thống, ta đếm số lượng labels khớp với giá trị dự đoán (prediction). Sau đó tính độ chính xác bằng cách tính giá trị trung bình của các kết quả trả về đúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctResult = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctResult, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó chúng ta sẽ xác định hàm độ lỗi sử dụng softmax cross entropy được tính từ dữ liệu dự đoán và tập labels. Cuối cùng là chọn thuật toán tối ưu với tham số learning rate mặc định là 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng Tensorboard để visualize kết quả"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong quá trình huấn luyện, chương trình sẽ ghi log về độ lỗi và độ chính xác trên tập train vào thư mục 'tensorboard', lưu lại model sau mỗi 2000 vòng lặp ở thư mục 'models'. Việc huấn luyện trên 30,000 vòng lặp mất khoảng vài tiếng với GPU K80 được cung cấp bởi Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với mỗi vòng lặp, ta sẽ lấy ra một batch dữ liệu train để đưa vào mạng sử dụng `feed_dict`. với các tham số input và label là các placeholders. Bước huấn luyện này được lặp lại cho đến khi hết số lần cần huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: 0.6854751110076904\n",
      "ep 50: 0.6839943528175354\n",
      "ep 100: 0.6752811074256897\n",
      "ep 150: 0.7411213517189026\n",
      "ep 200: 0.6825289726257324\n",
      "ep 250: 0.6765583753585815\n",
      "ep 300: 0.7003235220909119\n",
      "ep 350: 0.6957916021347046\n",
      "ep 400: 0.6911943554878235\n",
      "ep 450: 0.7028313279151917\n",
      "ep 500: 0.6664273738861084\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-19b78dfe24dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextBatchLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTrainBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Feed to optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnextBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnextBatchLabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#Write summary to Tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/VietAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/VietAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/VietAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/VietAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/VietAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/VietAI/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TODO 3.5\n",
    "    # Get next training batch\n",
    "    nextBatch, nextBatchLabels = getTrainBatch()\n",
    "    # Feed to optimizer\n",
    "    train_loss, _ = sess.run([loss, optimizer], feed_dict={inputs:nextBatch, labels:nextBatchLabels})\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {inputs: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "        print(\"ep {0}: {1}\".format(i, train_loss))\n",
    "\n",
    "    # Save model every 2000 training iterations\n",
    "    if ((i + 1) % 2000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, os.path.join(currentDir,\"models/pretrained_lstm.ckpt\"), global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load mô hình đã train và đánh giá mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thời gian huấn luyện mạng khá lâu, nên trong quá trình mạng đang được huấn luyện, ta sẽ lưu lại một số checkpoint. Để có thể test thử trên một checkpoint mới nhất ta sử dụng hàm tf.train.latest_checkpoint và truyền vào tên thư mục muốn lấy model mới nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/pretrained_lstm.ckpt-28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/anaconda3/envs/VietAI/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint(os.path.join(currentDir,'models')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, với mỗi batch dữ liệu test, ta sẽ tiến hành test và tính độ chính xác"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.6: Test mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 0.46875\n",
      "Accuracy for this batch: 0.4375\n",
      "Accuracy for this batch: 0.625\n",
      "Accuracy for this batch: 0.4375\n",
      "Accuracy for this batch: 0.53125\n",
      "Accuracy for this batch: 0.5625\n",
      "Accuracy for this batch: 0.515625\n",
      "Accuracy for this batch: 0.59375\n",
      "Accuracy for this batch: 0.453125\n",
      "Accuracy for this batch: 0.53125\n"
     ]
    }
   ],
   "source": [
    "# Test on 10 batches\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch()\n",
    "    # TODO 3.6: Tính độ chính xác 'accuracy' trên các test batch và gán vào 'test_acc'\n",
    "    test_acc = sess.run(accuracy, feed_dict={inputs:nextBatch, labels:nextBatchLabels})\n",
    "    print(\"Accuracy for this batch:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do các bộ test được lấy ngẫu nhiên nên độ chính xác trong quá trình này cũng dao động từ 70% đến 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.7: Viết hàm tổng hợp để dự đoán cảm xúc từ câu tiếng Việt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu cuối cùng này đòi hỏi đòi hỏi các bạn phải vận dụng tư duy tổng hợp để gom tất cả những bước đã thực hiện trước đó thành một quy trình hoàn chỉnh. Các bạn cần viết một hàm hoàn chỉnh với đầu vào là  một câu tiếng Việt cho trước, đầu ra là cho biết câu trên có cảm xúc tích cực hay tiêu cực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = 'Món này ăn ngon mê ly luôn. Vị ngọt và thơm quá trời quá đất.'\n",
    "# TODO 3.7 Các bạn vận dụng toàn bộ quy trình đã thực hiện trước đó\n",
    "# để dự đoán xem câu này có cảm xúc tích cực hay tiêu cực\n",
    "# Câu này làm khá dài và có tính chất tổng hợp\n",
    "\n",
    "def sentence_emotion(sentence):\n",
    "    # chuan hoa cau thanh vector 180 dong\n",
    "    cleanedLine = cleanSentences(sentence)\n",
    "    split = cleanedLine.split()\n",
    "    nIndexes = 0\n",
    "    sentence_vector = np.zeros((1, maxSeqLength), dtype='int32')\n",
    "    for word in split:\n",
    "        if word in wordsList:\n",
    "            sentence_vector[nIndexes] = wordsList.index(word)\n",
    "        else:\n",
    "            sentence_vector[nIndexes] = unk_idx\n",
    "\n",
    "        nIndexes = nIndexes + 1\n",
    "        if nIndexes >= maxSeqLength:\n",
    "            break\n",
    "            \n",
    "    # Feed data vao mang de tinh prediction\n",
    "    \n",
    "    \n",
    "    # tra ve ket qua\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy qua bài tập này, các bạn được ôn lại mô hình Word2Vec và sử dụng mô hình này để biểu diễn cho một văn bản. Sử dụng cách biểu diễn này để đưa vào mô hình RNN với nhiều đơn vị LSTM. Các bạn có thể thử nghiệm trên các cấu hình khác nhau bằng cách thay đổi các hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
